# General configuration defaults
beegfs_ha_ansible_cluster_group: ha_cluster                # Name for the pacemaker cluster.
beegfs_ha_ansible_storage_group: eseries_storage_systems   # Name of the Ansible E-Series storage group
beegfs_ha_cluster_name: hacluster                          # Pacemaker cluster group name
beegfs_ha_cluster_username: hacluster                      # Cluster group's username
beegfs_ha_cluster_password: hapassword                     # Cluster group's password
beegfs_ha_cluster_password_sha512_salt: randomSalt         # Cluster group's password salt
beegfs_ha_mgmtd_group: mgmt                                # BeeGFS HA management resource group
# beegfs_ha_mgmtd_floating_ip:                             # (Required) BeeGFS HA management resource floating IP address.
beegfs_ha_cluster_node_ips: []                             # Defines an order list of IP addresses or hostnames with the first having the highest priority. When
                                                           #    there are no listed IPs then node name will be used. Node names are defined in /etc/hosts.
beegfs_ha_firewall_start_and_enable: true  # Allow firewalld to start. This is required to create persistent port allowances. If the service is not running then it will be started and stopped once the configuration is set or removed.
beegfs_ha_firewall_allow_required_ports: true  # This allows all known and required ports.
beegfs_ha_firewall_zone: beegfs   # Default firewall zone for BeeGFS communications.
beegfs_ha_firewall_required_ha_ports:
 - [2224, tcp]  # pcs daemon
 - [3121, tcp]  # Pacemaker Remote nodes
 - [5405, udp]  # Corosync clustering - Required on all corosync nodes (needed by corosync)
#  - [5403, udp]  # Corosync quorum - Required on the quorum device host when using a quorum device with corosync-qnetd. The default value can be changed with the -p option of the corosync-qnetd command.
#  - [5404, udp]  # Corosync multicast - Required on corosync nodes if corosync is configured for multicast UDP
#  - [9929, tcp]  # Required to be open on all cluster nodes and booth arbitrator nodes to connections from any of those same nodes when the Booth ticket manager is used to establish a multi-site cluster.
#  - [9929, udp]  # Required to be open on all cluster nodes and booth arbitrator nodes to connections from any of those same nodes when the Booth ticket manager is used to establish a multi-site cluster.
#  - [21064, tcp] # DLM resources - Required on all nodes if the cluster contains any resources requiring DLM (such as clvm or GFS2)

# THESE CONFIGURATION OPTIONS ARE IN THE COROSYNC.CONF FILE
#beegfs_ha_enable_auto_tie_breaker: true
#beegfs_ha_enable_auto_tie_breaker_force_update: false
beegfs_ha_enable_quota: false
beegfs_ha_pacemaker_cluster_node_join_state: "default"  # This is the state cluster nodes will join the cluster (Choices: default, online, standby).
beegfs_ha_pacemaker_cluster_node: true  # Whether node should be a pacemaker cluster node. When set to false the node will be a pacemaker remote node. Note that corosync 2 only supports 32 cluster nodes. (Default: true)
beegfs_ha_pacemaker_enabled: false  # Whether pacemaker and corosync should be started by systemd during the boot process
beegfs_ha_filter_ip_ranges: []
beegfs_ha_node_preference_scope_step: 200

# Alerts (Postfix) service defaults
beegfs_ha_enable_alerts: true
beegfs_ha_alert_email_list: []  # List of emails to alert when changes occur.
beegfs_ha_alert_conf: /etc/postfix/main.cf
beegfs_ha_alert_conf_options: {}
beegfs_ha_alert_conf_ha_group_options: {}
beegfs_ha_alert_conf_default_options:
  myorigin: $mydomain
  inet_interfaces: all
  inet_protocols: all
  mydestination: $myhostname, localhost.$mydomain, localhost, $mydomain
  home_mailbox: Maildir/
beegfs_ha_alert_sender: noreply
beegfs_ha_alert_timestamp_format: "%Y-%m-%d %H:%M:%S.%06N"
beegfs_ha_alert_verbosity: 3  #  1) high-level node activity
                              #  3) high-level node activity + fencing action information + resources (filter on X-monitor)
                              #  5) high-level node activity + fencing action information + resources

beegfs_ha_cluster_crm_config_options: {}  # Dictionary for overriding or adding to the beegfs_ha_cluster_crm_config_defaults.
beegfs_ha_cluster_crm_config_defaults: # WARNING! These defaults are required for high-availability.
  symmetric-cluster: "false"  # Opt-in architecture
  fence-reaction: panic     # Allows cluster nodes with a fence resource to fence itself by forcing a kernel panic.
  concurrent-fencing: "true"  # Allows cluster nodes to be fenced concurrently.
  cluster-ipc-limit: 5500   # The maximum IPC message backlog before one cluster daemon will disconnect another. This is of use in large clusters, for which a good value is the number of resources in the cluster multiplied by the number of nodes.
  start-failure-is-fatal: "false"  # Whether a failure to start a resource on a particular node prevents further start attempts on that node?
  stonith-action: "off"       # Action the fencing agent takes when stonith-enabled is set to true. Options: off, reboot
  stonith-enabled: "true"     # When stonith-enabled is set to false then beegfs-monitor monitoring operation's on-fail action will be set to standby instead of fence.

beegfs_ha_cluster_crm_config_ignore:
  - cluster-infrastructure
  - dc-version
  - cluster-name
  - maintenance-mode

beegfs_ha_cluster_resource_defaults:
  resource-stickiness: 15000
beegfs_ha_fencing_agents: {}
beegfs_ha_backup: true
beegfs_ha_backup_path: /tmp/
beegfs_ha_storage_system_hostgroup_prefix: beegfs
beegfs_ha_force_resource_move: true   # Forces node and resource changes to migrate services to preferred nodes.

# BeeGFS pacemaker resource configuration defaults
beegfs_ha_resource_agent_path: /usr/lib/ocf/resource.d/
beegfs_ha_resource_monitor_monitor_interval: 10s     # BeeGFS monitoring service monitor interval - Time interval from end of monitor action call to the start of another monitor action call.
beegfs_ha_resource_monitor_monitor_timeout: 60s      # BeeGFS monitoring service monitor timeout - Maximum time allowed for the monitor action call before failing.
beegfs_ha_resource_monitor_start_timeout: 240s       # The monitored BeeGFS ipaddr, filesystem, and systemd service must start within this timeframe.
beegfs_ha_resource_monitor_stop_timeout: 30s         # This is only the timeout for beegfs_monitor_stop() to complete (not the BeeGFS resource group).
beegfs_ha_resource_monitor_migration_threshold: 1    # BeeGFS monitoring service failures before migrating away from failed node.
beegfs_ha_resource_filesystem_monitor_interval: 17s  # ocf:heartbeat:Filesystem resource monitor operation interval
beegfs_ha_resource_filesystem_stor_start_timeout: 180s # After an unclean unmount it make take 35s+ to run XFS journal recovery on a busy system.
beegfs_ha_resource_filesystem_stop_timeout: 30s      # ocf:heartbeat:Filesystem resource stop operation interval

beegfs_ha_resource_ipaddr_monitor_interval: 17s      # ocf:eseries:beegfs-ipaddr2 resource monitor operation interval
beegfs_ha_resource_mailto_monitor_interval: 60s      # ocf:heartbeat:mailTo resource monitor operation interval
beegfs_ha_resource_systemd_monitor_interval: 13s     # ocf:heartbeat:systemd resource monitor operation interval
beegfs_ha_resource_systemd_mgmt_stop_timeout: 245s   # These should be at least `systemctl show <SERVICE> -p TimeoutStopUSec` + 5.
beegfs_ha_resource_systemd_meta_stop_timeout: 95s      # Note TimeoutStopUSec can be overridden in the systemd unit files using TimeoutStopSec.
beegfs_ha_resource_systemd_stor_stop_timeout: 95s      # Currently meta/storage services have the same TimeoutStopUSec, but it could change.
beegfs_ha_resource_systemd_migration_threshold: 3    # BeeGFS service failures before migrating away from failed node.

# Niceness configuration defaults
beegfs_ha_pacemaker_service_niceness: -12
beegfs_ha_beegfs_mgmtd_service_niceness: -3
beegfs_ha_beegfs_meta_service_niceness: -2
beegfs_ha_beegfs_storage_service_niceness: -1
beegfs_ha_beegfs_monitor_niceness: -10
beegfs_ha_beegfs_filesystem_niceness: -15

# Performance tuning defaults
beegfs_ha_enable_performance_tuning: false
beegfs_ha_eseries_nvme_attributes:
  queue/scheduler: none
  queue/read_ahead_kb: 4096
  queue/max_sectors_kb: 1024
  queue/nomerges: 2
  queue/rq_affinity: 1
beegfs_ha_eseries_ssd_attributes:
  queue/scheduler: noop
  queue/nr_requests: 64
  queue/read_ahead_kb: 4096
  queue/max_sectors_kb: 1024
  queue/nomerges: 2
  queue/rq_affinity: 1
beegfs_ha_eseries_hdd_attributes:
  queue/scheduler: deadline
  queue/nr_requests: 1024
  queue/read_ahead_kb: 4096
  queue/max_sectors_kb: 1024
  queue/nomerges: 2
  queue/rq_affinity: 1
beegfs_ha_eseries_hdd_queue_attributes:
  scheduler: mq-deadline
  nr_requests: 64
  read_ahead_kb: 4096
  max_sectors_kb: 1024
  nomerges: 2
  rq_affinity: 1

beegfs_ha_sysfs_devpath_device_attributes:  # Set any path attributes relative to sysfs /sys directory.
  kernel/mm/transparent_hugepage:
    defrag: always

# Performance sysctl options
beegfs_ha_performance_sysctl_entries:
  vm.dirty_background_ratio: 1
  vm.dirty_ratio: 75
  vm.vfs_cache_pressure: 50
  vm.min_free_kbytes: 262144
  vm.zone_reclaim_mode: 1
  vm.watermark_scale_factor: 1000

# Required sysctl options
beegfs_ha_required_sysctl_entries:
  net.ipv4.conf.all.rp_filter: 1
  net.ipv4.conf.all.arp_filter: 1
  net.ipv4.conf.all.arp_announce: 2
  net.ipv4.conf.all.arp_ignore: 2

# BeeGFS management service configuration
beegfs_ha_beegfs_mgmtd_conf_path: /etc/beegfs/beegfs-mgmtd.conf   # Default BeeGFS management service configuration file path.
beegfs_ha_beegfs_mgmtd_conf_resource_group_options: {}            # Ansible resource group specific options. For BeeGFS management resource specific configuration.
beegfs_ha_beegfs_mgmtd_conf_ha_group_options: {}                  # Ansible cluster group specific options. For all common or expected management configuration defaults.
beegfs_ha_beegfs_mgmtd_conf_default_options:                      # Default management configuration options.
  storeAllowFirstRunInit: "false" # The quotes prevent the value from being treated like a boolean which converts to True/False.
  connMgmtdPortTCP: 8008
  connMgmtdPortUDP: 8008
  sysTargetOfflineTimeoutSecs: 900 # This is required to avoid the mgmt service prematurely placing targets offline if the preferred meta/storage interface fails (ESOLA-116).

# BeeGFS metadata service configuration
beegfs_ha_beegfs_meta_conf_path: /etc/beegfs/beegfs-meta.conf   # Default BeeGFS metadata service configuration file path.
beegfs_ha_beegfs_meta_conf_resource_group_options: {}           # Ansible resource group specific options. For BeeGFS metadata resource specific configuration.
beegfs_ha_beegfs_meta_conf_ha_group_options: {}                 # Ansible cluster group specific options. For all common or expected metadata configuration defaults.
beegfs_ha_beegfs_meta_conf_default_options:                     # Default metadata configuration options.
  storeAllowFirstRunInit: "false"
  tuneTargetChooser: randomized
  connMaxInternodeNum: 128
  connMetaPortTCP: 8005
  connMetaPortUDP: 8005
  connMgmtdPortTCP: 8008
  connMgmtdPortUDP: 8008
  tuneNumWorkers: 32
  connUseRDMA: "true"
  sysTargetOfflineTimeoutSecs: 900 # This is required to avoid the mgmt service prematurely placing targets offline if the preferred meta/storage interface fails (ESOLA-116).

# BeeGFS storage service configuration
beegfs_ha_beegfs_storage_conf_path: /etc/beegfs/beegfs-storage.conf   # Default BeeGFS storage service configuration file path.
beegfs_ha_beegfs_storage_conf_resource_group_options: {}              # Ansible resource group specific options. For BeeGFS storage resource specific configuration.
beegfs_ha_beegfs_storage_conf_ha_group_options: {}                    # Ansible cluster group specific options. For all common or expected storage configuration defaults.
beegfs_ha_beegfs_storage_conf_default_options:                        # Default storage configuration options.
  storeAllowFirstRunInit: "false"
  connMaxInternodeNum: 128
  connMgmtdPortTCP: 8008
  connMgmtdPortUDP: 8008
  connStoragePortTCP: 8003
  connStoragePortUDP: 8003
  tuneNumStreamListeners: 2
  tuneNumWorkers: 14
  tuneUseAggressiveStreamPoll: "true"
  tuneFileReadSize: 2048k
  tuneFileWriteSize: 2048k
  connUseRDMA: "true"
  sysTargetOfflineTimeoutSecs: 900 # This is required to avoid the mgmt service prematurely placing targets offline if the preferred meta/storage interface fails (ESOLA-116).

# Journald logging defaults
beegfs_ha_journald_allow_changes: true                    # Allows changes to be made to journald configuration changes.
beegfs_ha_journald_options:                               # Default journald configuration options. These key-value pairs will be applied to the journal.conf file.
  Storage: persistent
  Compress: "true"
  SystemMaxUse: 50G
  SystemKeepFree: 10G
  SystemMaxFileSize: 256M
  SystemMaxFiles: 2000

# NTP configuration defaults
beegfs_ha_ntp_configuration_file: /etc/ntp.conf
beegfs_ha_chrony_configuration_file: /etc/chrony.conf
beegfs_ha_chrony_driftfile_file: /var/lib/chrony/drift
beegfs_ha_chrony_keyfile_file: /etc/chrony.keys
beegfs_ha_ntp_server_pools:
  - "pool pool.ntp.org iburst"
beegfs_ha_ntp_restricts:
  - 127.0.0.1
  - ::1

# Pacemaker defaults
beegfs_ha_pacemaker_path: /etc/pacemaker/
beegfs_ha_pacemaker_authkey_path: /etc/pacemaker/authkey
beegfs_ha_pacemaker_config_path: /etc/sysconfig/pacemaker
beegfs_ha_pacemaker_cib_path: /var/lib/pacemaker/cib/
beegfs_ha_pacemaker_ipc_buffer_bytes: 131072

# Corosync defaults
beegfs_ha_corosync_authkey_path: /etc/corosync/authkey
beegfs_ha_corosync_conf_path: /etc/corosync/corosync.conf
beegfs_ha_corosync_log_path: /var/log/corosync.log

# Pcs defaults
beegfs_ha_pcsd_path: /var/lib/pcsd/

# Package version defaults
beegfs_ha_force_upgrade: false
#beegfs_ha_beegfs_version:  # By default the BeeGFS version will be selected based on the repository url (beegfs_ha_X_repository_base_url).
beegfs_ha_corosync_version: 3.1.5
beegfs_ha_pacemaker_version: 2.1.0
beegfs_ha_pacemaker_remote_version: 2.1.0
beegfs_ha_pcs_version: 0.10.10
beegfs_ha_resource_agent_version: 4.1.1
beegfs_ha_fence_agents_all_version: 4.2.1

# Uninstall defaults
beegfs_ha_uninstall: false                                          # Whether to uninstall the entire BeeGFS HA solution excluding the storage provisioning and host storage setup.
beegfs_ha_uninstall_unmap_volumes: false                            # Whether to unmap the volumes from the host only. This will not effect the data.
beegfs_ha_uninstall_wipe_format_volumes: false                      # Whether to wipe format signatures from volumes on the host. **WARNING! This action is unrecoverable.**
beegfs_ha_uninstall_delete_volumes: false                           # Whether to delete the volumes from the storage. **WARNING! This action is unrecoverable.**
beegfs_ha_uninstall_delete_storage_pools_and_host_mappings: false   # Whether to delete all storage pools/volume groups and host/host group mappings created for BeeGFS HA solution.
                                                                    #   Be aware that removing storage pools/volume groups and host/host group mappings will effect any volumes or
                                                                    #   host mappings dependent on them.  **WARNING! This action is unrecoverable.**
beegfs_ha_uninstall_storage_setup: false                            # Whether to remove configuration that allows storage to be accessible to the BeeGFS HA node
beegfs_ha_uninstall_reboot: true                                    # Whether to reboot after uninstallation.

# Volume formatting and mounting defaults
beegfs_ha_service_volume_configuration:
  management:
    format_type: ext4
    format_options: "-i 2048 -I 512 -J size=400 -Odir_index,filetype"
    mount_options: "sync,noatime,nodiratime,nobarrier"
    mount_dir: /mnt/
  metadata:
    format_type: ext4
    format_options: "-i 2048 -I 512 -J size=400 -Odir_index,filetype"
    mount_options: "sync,noatime,nodiratime,nobarrier"
    mount_dir: /mnt/
  storage:
    format_type: xfs
    format_options: "-d su=VOLUME_SEGMENT_SIZE_KBk,sw=VOLUME_STRIPE_COUNT -l version=2,su=VOLUME_SEGMENT_SIZE_KBk"
    mount_options: "sync,noatime,nodiratime,logbufs=8,logbsize=256k,largeio,inode64,swalloc,allocsize=131072k"  # Note that nobarrier does not work on newer versions of XFS (ie. RHEL8)
    mount_dir: /mnt/

# Debian / Ubuntu repository defaults
beegfs_ha_debian_repository_base_url: https://www.beegfs.io/release/beegfs_7.2.6
beegfs_ha_debian_repository_gpgkey: https://www.beegfs.io/release/beegfs_7.2.6/gpg/GPG-KEY-beegfs


# RedHat / CentOS repository defaults
beegfs_ha_rhel_repository_base_url: https://www.beegfs.io/release/beegfs_7.2.6/dists/rhel8
beegfs_ha_rhel_repository_gpgkey: https://www.beegfs.io/release/beegfs_7.2.6/gpg/GPG-KEY-beegfs


# SUSE repository defaults
beegfs_ha_suse_allow_unsupported_module: true
beegfs_ha_suse_repository_base_url: https://www.beegfs.io/release/beegfs_7.2.6/dists/sles15
beegfs_ha_suse_repository_gpgkey: https://www.beegfs.io/release/beegfs_7.2.6/gpg/GPG-KEY-beegfs


# Patterns for top-level variables collected from cluster group inventory to be added to each resource inventory.
#   The resource inventories are generated at the beginning of the role's execution. High-availability requires
#   resources to be available on multiple hosts these patterns help select what information is used.
beegfs_ha_top_level_variable_exclusion_pattern: "^ansible.*"
beegfs_ha_top_level_variable_inclusion_pattern: "^(beegfs|eseries).*"

# netapp_eseries.host collection defaults.
eseries_common_force_skip_uninstall: True
