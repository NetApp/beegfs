{%- set iface = example_interface -%}
# {{ item['item']['item'] }} - BeeGFS HA Resource Group

# This is a generic template for BeeGFS HA resource groups. You are seeing this for a resource due to not provided a beegfs service or an invalid one during the
#   create_inventory_structure.yml playbook runtime. Review all comments and update with all expected values. All capitalized words in angled brackets must be updated.

### IMPORTANT: Choose one of the below beegfs_ha_beegfs_[mgmtd|meta|storage]_conf_resource_group_options and delete the others:

beegfs_ha_beegfs_mgmtd_conf_resource_group_options:
 connMgmtdPortTCP: 8008 # BeeGFS HA resource service port. 8008 is the default BeeGFS management service port number.
 connMgmtdPortUDP: 8008

beegfs_ha_beegfs_meta_conf_resource_group_options:
  connMetaPortTCP: <UNIQUE_PORT_NUMBER> # BeeGFS HA resource service port. 8005 is the default BeeGFS metadata service port number; Each BeeGFS HA metadata
  connMetaPortUDP: <UNIQUE_PORT_NUMBER> #   service resource must have a unique port number
  tuneBindToNumaZone: <NUMA_NODE>       # NUMA node binding for BeeGFS service. This should be defined at the resource group level. This NUMA policy will be applied whenever and
                                        #   wherever the BeeGFS service resides. This can provide significant performance increases by avoiding the cache misses between NUMA nodes.

beegfs_ha_beegfs_storage_conf_resource_group_options:
  connStoragePortTCP: <UNIQUE_PORT_NUMBER> # BeeGFS HA resource service port. 8012 is the default BeeGFS storage service port number; Each BeeGFS HA
  connStoragePortUDP: <UNIQUE_PORT_NUMBER> #   storage service resource must have a unique port number.
  tuneBindToNumaZone: <NUMA_NODE>          # NUMA node binding for BeeGFS service. This should be defined at the resource group level. This NUMA policy will be applied whenever and
                                           #   wherever the BeeGFS service resides. This can provide significant performance increases by avoiding the cache misses between NUMA nodes.

### IMPORTANT: Choose one of the above beegfs_ha_beegfs_[mgmtd|meta|storage]_conf_resource_group_options and delete the others.

floating_ips:                               # List of floating IP definitions in the form host_interface:ip_address/cidr_subnet_mask (i.e. eth0:192.168.2.102/24).
  - <INTERFACE:ADDRESS/CIDR_SUBNET_MASK>     #   This information is used to communicate with the BeeGFS management service. Therefore, these definitions must
                                            #   be on the same subnet as beegfs_ha_mgmtd_floating_ip value in group_vars/ha_cluster.yml. When defining a
                                            #   management service, be sure that the addresses correspond to beegfs_ha_mgmtd_floating_ip. Any unusable definitions
                                            #   will be ignored which is specially useful when the definitions differ based on the host.

beegfs_service: <BEEGFS_SERIVCE>            # Type of BeeGFS service the HA resource group will manage. Choices: management, metadata, storage.

beegfs_targets:                             # Dictionary of E-Series storage systems that contain a list of storage pool definitions.
  {{ storage_target_hosts[0] }}:            # E-Series storage target. Change this to any desired E-Series storage target found in
                                            #   eseries_storage_systems inventory group. Choices: ({% for storage_host_target in storage_target_hosts %}{{ storage_host_target }}{% if not loop.last %},{% endif %}{% endfor %}).

    eseries_storage_pool_configuration:     # Storage pool and volume names are generated by the beegfs_ha_7_3 role. For more options, see examples/beegfs_ha_7_3/README.md.

      - #name: mgmt_meta_vg                 # The management service does not require much storage. For this reason the storage pool's name is specified here to
                                            #   provide a common pool of drives to be used between this metadata services and the management service. If name
                                            #   is changed, differs from the management storage pool's name or removed a common storage pool will not be created.

        raid_level: <RAID_LEVEL>            # Storage pool/volume group RAID type. Choices: raid0, raid1, raid5, raid6, raidDiskPool
                                            #   RAID 1 mirrors the data between two drives which is the recommended storage type for management and metadata
                                            #   services. RAID 6 stripes the data between across all drives with two parity stripes to allow for recovery of
                                            #   multiple drive failures which is the recommended storage type for BeeGFS services. If this storage pool is shared
                                            #   with other services this value must be the same on all definitions.

        criteria_drive_count: <DRIVE_COUNT> # Total drive count for the storage pool. RAID 1 requires even number of drives. If this storage pool is shared with
                                            #   other services this value must be the same on all definitions.
        volumes:                            # BeeGFS metadata service requires only one volume.
          - size: <VOLUME_SIZE_PCT>         # Size of the BeeGFS metadata service volume specified in percentage of total storage pool capacity.
                                            # Depending on your configuration, adjust the percentage to account for overprovisioning.
